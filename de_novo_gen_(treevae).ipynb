{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3ab7f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"light_chembl_smiles_30.csv\")\n",
    "\n",
    "def clean_fragment_sequence(seq_str):\n",
    "    frag_list = ast.literal_eval(seq_str)\n",
    "    return [frag.strip().strip(\"'\\\"\") for frag in frag_list]\n",
    "\n",
    "df['fragments'] = df['fragments'].apply(clean_fragment_sequence)\n",
    "\n",
    "# Convert fragment string back to list\n",
    "#df['fragments'] = df['fragments'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c8e66cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1*]C(=O)C(C([1*])=O)=C1SC=CS1', '[3*]O[3*]', '[4*]C(C)C']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fragments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "45829d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fragments in vocabulary: 311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[3*]O[3*]',\n",
       " '[4*]C(C)C',\n",
       " '[4*]C[8*]',\n",
       " '[11*]SC',\n",
       " '[3*]OC',\n",
       " '[1*]C([6*])=O',\n",
       " '[16*]c1ccccc1Cl',\n",
       " '[16*]c1ccccc1',\n",
       " '[4*]C(C)C[8*]',\n",
       " '[5*]N[5*]']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Flatten all fragment lists into a single list\n",
    "all_fragments = [frag for sublist in df['fragments'] for frag in sublist]\n",
    "\n",
    "# Count occurrences (optional, can be used to prune rare fragments)\n",
    "frag_counter = Counter(all_fragments)\n",
    "\n",
    "# Optional: filter fragments that occur very rarely (e.g., < 2 times)\n",
    "min_count = 10\n",
    "vocab_fragments = [frag for frag, count in frag_counter.items() if count >= min_count]\n",
    "\n",
    "print(f\"Number of fragments in vocabulary: {len(vocab_fragments)}\")\n",
    "vocab_fragments[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e1926707",
   "metadata": {},
   "outputs": [],
   "source": [
    "frag2idx = {frag: i for i, frag in enumerate(vocab_fragments)}\n",
    "idx2frag = {i: frag for frag, i in frag2idx.items()}\n",
    "\n",
    "# Example: convert fragments of a molecule to indices\n",
    "df['fragment_indices'] = df['fragments'].apply(lambda frags: [frag2idx[f] for f in frags if f in frag2idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9d58edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[3*]O[3*]': 0, '[4*]C(C)C': 1, '[4*]C[8*]': 2, '[11*]SC': 3, '[3*]OC': 4, '[1*]C([6*])=O': 5, '[16*]c1ccccc1Cl': 6, '[16*]c1ccccc1': 7, '[4*]C(C)C[8*]': 8, '[5*]N[5*]': 9, '[6*]C([6*])=O': 10, '[16*]c1cccc([16*])c1': 11, '[8*]C(C)C(=O)O': 12, '[8*]C(F)(F)F': 13, '[16*]c1ccc([16*])cc1': 14, '[4*]CC': 15, '[5*]N1CCC[C@H]1[13*]': 16, '[8*]CC': 17, '[6*]C(=O)O': 18, '[16*]c1ccncc1': 19, '[8*]C[8*]': 20, '[6*]C(=O)CO': 21, '[5*]N1CCN([5*])CC1': 22, '[7*]CC[7*]': 23, '[4*]C(C)C([8*])O': 24, '[16*]c1ccc(O)cc1': 25, '[4*]CC([4*])C': 26, '[4*]CCC': 27, '[4*]CC(O)C[4*]': 28, '[4*]CC[4*]': 29, '[5*]N([5*])[5*]': 30, '[9*]n1cncn1': 31, '[16*]c1ccc(F)cc1F': 32, '[1*]C(=O)N(CCCl)N=O': 33, '[16*]c1cnc(C)nc1N': 34, '[16*]c1ccc(Cl)cc1': 35, '[16*]c1cnn(C)c1': 36, '[6*]C(N)=O': 37, '[14*]c1ncccc1[16*]': 38, '[16*]c1cccnc1': 39, '[16*]c1ccccc1[16*]': 40, '[8*]CO': 41, '[11*]S[11*]': 42, '[15*]C1CC1': 43, '[5*]N(C)C': 44, '[15*]C1CCCCC1': 45, '[1*]C(=O)CC[8*]': 46, '[5*]N1CCOCC1': 47, '[14*]c1ccco1': 48, '[5*]N([5*])C': 49, '[4*]CCC[4*]': 50, '[13*]C1CCCO1': 51, '[4*]CCCCCC': 52, '[4*]CCCC': 53, '[4*]CCCC([6*])=O': 54, '[16*]c1ccc(F)cc1': 55, '[10*]N1CCCC1=O': 56, '[16*]c1c(I)cc(I)c([16*])c1I': 57, '[16*]c1ccc2c(c1)OCO2': 58, '[1*]C(=O)C[7*]': 59, '[5*]N1CCCCC1': 60, '[7*]C[8*]': 61, '[4*]C(C)(C)C': 62, '[16*]c1ccc(I)cc1': 63, '[8*]C[C@H](N)C(=O)O': 64, '[4*]CC[8*]': 65, '[16*]c1ccc([16*])c([16*])c1': 66, '[7*]C([8*])C': 67, '[16*]c1cccc(Cl)c1Cl': 68, '[13*]C1Nc2cc(Cl)c(S(N)(=O)=O)cc2S(=O)(=O)N1': 69, '[1*]C(=O)CC': 70, '[15*][C@H]1CN2CCC1CC2': 71, '[4*]CC=C': 72, '[14*]c1ncccn1': 73, '[1*]C(C)=O': 74, '[16*]c1ccccc1O': 75, '[1*]C(=O)C([8*])([8*])O': 76, '[8*]CN': 77, '[14*]c1cccs1': 78, '[5*]N1CCCC1': 79, '[1*]C(=O)C([8*])CO': 80, '[16*]c1cccc(F)c1': 81, '[9*]n1ccc(N)nc1=O': 82, '[4*]CC#C': 83, '[16*]c1ccc(C)cc1': 84, '[6*]C(=O)CC[8*]': 85, '[6*]C(C)=O': 86, '[16*]c1ccccc1F': 87, '[3*]OP(=O)(O)O': 88, '[14*]c1ccc([16*])cn1': 89, '[5*]NC': 90, '[16*]c1cccc(C)c1': 91, '[16*]c1ccccc1C': 92, '[16*]c1c[nH]c2ccccc12': 93, '[16*]c1ccc([16*])c(F)c1': 94, '[8*]C(C)C': 95, '[16*]c1ccc(Cl)c(Cl)c1': 96, '[8*]CCC(=O)O': 97, '[16*]c1cc(I)c([16*])c(I)c1': 98, '[12*]S(=O)(=O)c1ccc([16*])cc1': 99, '[14*]c1c[nH]cn1': 100, '[4*][C@H](C)C[8*]': 101, '[16*]c1ccc(Br)cc1': 102, '[4*]CCC([8*])[8*]': 103, '[14*]c1ccccn1': 104, '[9*]n1cnc2c1c(=O)n(C)c(=O)n2C': 105, '[5*]N1CCC([15*])CC1': 106, '[12*]S(C)(=O)=O': 107, '[1*]C([1*])=O': 108, '[4*]C([8*])[8*]': 109, '[1*]C(=O)C(Cl)Cl': 110, '[16*]c1ccc(S(C)(=O)=O)cc1': 111, '[14*]c1nc2ccc([16*])cc2s1': 112, '[4*]C([8*])([8*])C': 113, '[16*]c1cc(I)c(O)c(I)c1': 114, '[4*]C(C)CC[8*]': 115, '[1*]C(=O)[C@@H]([8*])CO': 116, '[16*]c1ccc(Cl)cc1Cl': 117, '[9*]n1ccnc1': 118, '[4*]C([8*])C[8*]': 119, '[14*]c1cnccn1': 120, '[8*]CC=C': 121, '[16*]c1ccc(O)c(O)c1': 122, '[16*]c1cc(N)c(Cl)cc1[16*]': 123, '[5*][N+]([5*])(C)C': 124, '[4*]CCCCCCCCCCCC': 125, '[1*]C(=O)CCC(=O)O': 126, '[10*]N1C[C@H]([13*])OC1=O': 127, '[16*]c1c(Cl)cccc1Cl': 128, '[5*]N1c2ccccc2Sc2ccccc21': 129, '[9*]n1cc(C)c(=O)[nH]c1=O': 130, '[15*]C1CCCC1': 131, '[13*]C1SC(=O)NC1=O': 132, '[4*]CCCCC': 133, '[14*]c1ncc([16*])cn1': 134, '[4*]CC([8*])O': 135, '[15*]C1CN2CCC1CC2': 136, '[16*]c1cc([16*])cc([16*])c1': 137, '[4*][C@@H](CC[8*])C(=O)O': 138, '[1*]C(=O)[C@@H]([4*])C': 139, '[14*]c1nnn[nH]1': 140, '[3*]OC[8*]': 141, '[1*]C(=O)[C@@H](N)C(C)C': 142, '[5*]N1c2ccccc2Sc2ccc(Cl)cc21': 143, '[8*]C(C)(C)C': 144, '[4*]CC1=C(C(=O)O)N2C(=O)[C@@H]([15*])[C@H]2SC1': 145, '[16*]c1cc([16*])c([16*])c([16*])c1': 146, '[5*][N+]([5*])([5*])[5*]': 147, '[5*][N+]1(C)[C@@H]2CC[C@H]1C[C@@H]([15*])C2': 148, '[7*]C([8*])[8*]': 149, '[8*]CCC': 150, '[16*]c1ccc(F)c([16*])c1': 151, '[12*]S(=O)(=O)c1ccc(N)cc1': 152, '[16*]c1cccc(Cl)c1': 153, '[5*][N+](C)(C)C': 154, '[16*]c1ccc([16*])c(C)c1': 155, '[4*]CC(=O)O': 156, '[4*]CC([4*])C[4*]': 157, '[1*]C(=O)CCC': 158, '[8*]CC(=O)O': 159, '[8*]CCCCC': 160, '[1*]C(=O)C[8*]': 161, '[5*]NC(=N)N': 162, '[16*]c1ccc([16*])c(O)c1': 163, '[4*]CC[7*]': 164, '[16*]c1ccc2cc([16*])ccc2c1': 165, '[3*]OS(C)(=O)=O': 166, '[4*]CCCC[4*]': 167, '[7*]CC1=C(C)CCCC1(C)C': 168, '[7*]CC([7*])C': 169, '[7*]CC(=O)O': 170, '[4*]C[C@@H]([8*])O': 171, '[4*]CCCCCC[4*]': 172, '[16*]c1cccc2ccccc12': 173, '[14*]c1ccc([N+](=O)[O-])o1': 174, '[1*]C(=O)C[4*]': 175, '[1*]C(N)=O': 176, '[14*]c1nccs1': 177, '[16*]c1ccc(N)cc1': 178, '[15*][C@H]1C[C@H]2CC[C@@H](C1)N2C': 179, '[7*]CC': 180, '[4*]CC(CC)CCCC': 181, '[5*]N1c2ccccc2Sc2ccc([16*])cc21': 182, '[5*]N1CCN(C)CC1': 183, '[1*]C(=O)CN': 184, '[3*]OC(F)(F)F': 185, '[7*]C1CCC[C@]2(C)[C@@H]([15*])CC[C@@H]12': 186, '[16*]c1ccc2[nH]cc([16*])c2c1': 187, '[4*]CC([6*])=O': 188, '[5*][N+]([5*])([5*])C': 189, '[16*]c1ccc2ccccc2c1': 190, '[7*]CCCCCCCCC': 191, '[14*]c1nc2cc([16*])ccc2[nH]1': 192, '[3*]OC(F)F': 193, '[4*]CCO': 194, '[4*]CCCC[8*]': 195, '[16*]c1c(C)cccc1C': 196, '[16*]c1cc(C)c([16*])c(C)c1': 197, '[4*]C1([15*])CCN([5*])CC1': 198, '[5*]N1CCC([15*])([15*])CC1': 199, '[16*]c1ccc(O)c([16*])c1': 200, '[15*]C1([15*])C(=O)NC(=O)NC1=O': 201, '[4*]CC(C)C[4*]': 202, '[5*]N1CCCC1[13*]': 203, '[3*]O[N+](=O)[O-]': 204, '[5*][N-][5*]': 205, '[4*]CC(C)C': 206, '[1*]C(=O)C([8*])[8*]': 207, '[14*]c1nc[nH]c1C': 208, '[1*]C(=O)C1=C(C)NC(C)=C(C([1*])=O)C1[15*]': 209, '[1*]C(=O)C(C)(C)C': 210, '[8*]C([8*])C': 211, '[8*]CCCC': 212, '[16*]c1cccc(O)c1': 213, '[4*]CC1=NCCN1': 214, '[6*]C(=O)[O-]': 215, '[8*]CCO': 216, '[15*][C@H]1CC[C@H]([15*])CC1': 217, '[1*]C(=O)C([4*])(C)C': 218, '[16*]c1ccc(Cl)c([16*])c1': 219, '[5*]NC1=NCCN1': 220, '[16*]c1ccc(Cl)cc1[16*]': 221, '[7*]C(C)C': 222, '[4*]CCC([8*])([8*])O': 223, '[7*]CCC[7*]': 224, '[4*]CCC[8*]': 225, '[15*]C12CC3CC(CC(C3)C1)C2': 226, '[5*]N1CCNCC1': 227, '[6*]C(=O)CC': 228, '[9*]n1cnc2c(N)ncnc21': 229, '[4*]CC(=O)[O-]': 230, '[7*]C1CCN(C)CC1': 231, '[7*]CC[8*]': 232, '[4*][C@@H](C)C[8*]': 233, '[8*][C@@H](C)O': 234, '[4*]CCCCCCCCCCCCCCCC': 235, '[8*]CC(C)C': 236, '[8*]C([8*])[8*]': 237, '[12*]S(=O)(=O)c1ccc(C)cc1': 238, '[13*][C@@H]1N2C(=O)[C@@H]([15*])[C@H]2SC1(C)C': 239, '[1*]C(=O)C([4*])C': 240, '[1*]C(=O)C([8*])C': 241, '[8*]C(C)CCC': 242, '[15*]C1CCC1': 243, '[14*]c1ccc([14*])o1': 244, '[4*]C(C)(C)C[8*]': 245, '[9*]n1cc([16*])c(=O)c2cc(F)c([16*])cc21': 246, '[14*]c1csc([14*])n1': 247, '[16*]c1ccc(C#N)cc1': 248, '[4*][C@H]([8*])C': 249, '[4*][C@@H]([8*])C': 250, '[4*]CCCC(=O)O': 251, '[14*]c1ncc(C)c([16*])c1C': 252, '[14*]c1ncnc2cc([16*])c([16*])cc12': 253, '[16*]c1ccc([16*])c(Cl)c1': 254, '[15*]C1CCN(C)CC1': 255, '[4*]CC(F)(F)F': 256, '[9*]n1c([N+](=O)[O-])cnc1C': 257, '[13*][C@@H]1O[C@H]([13*])[C@@H](O)[C@@H]1O': 258, '[4*]CCCl': 259, '[13*][C@@H]1O[C@H]([13*])[C@@H](O)[C@H]1O': 260, '[14*]c1cc([16*])ccn1': 261, '[1*]C(=O)CC[4*]': 262, '[4*]CCC[7*]': 263, '[16*]c1cccc([16*])c1[16*]': 264, '[15*]C1CCNCC1': 265, '[14*]c1noc([14*])n1': 266, '[4*]CCCCCCCC': 267, '[1*]C(=O)C([4*])[8*]': 268, '[5*][N+]1(C)CCCC1': 269, '[16*]c1ccc(S(N)(=O)=O)cc1': 270, '[16*]c1ccc([N+](=O)[O-])cc1': 271, '[1*]C(=O)C(C)C': 272, '[6*]C(=O)NO': 273, '[9*]n1cnc2c(=O)[nH]c(N)nc21': 274, '[4*]CCN': 275, '[4*]CC(O)CO': 276, '[16*]c1ccnc2cc(Cl)ccc12': 277, '[4*]C(C)(C)C(=O)O': 278, '[16*]c1c(I)c([16*])c(I)c([16*])c1I': 279, '[9*]n1c(=O)c([16*])c(C)n1C': 280, '[4*]C[C@H](N)C(=O)O': 281, '[14*]c1ccc([14*])nn1': 282, '[7*]C1c2ccccc2CCc2ccccc21': 283, '[8*]CC[8*]': 284, '[4*]CCC(C)C': 285, '[1*]C(=O)[C@@H]([8*])N': 286, '[4*]CCC([8*])([8*])C(N)=O': 287, '[11*]SS[11*]': 288, '[13*]C1CCCCN1': 289, '[8*]CC1=NCCN1': 290, '[11*]SC(C)=O': 291, '[15*][C@H]1[C@H](O)CC(=O)[C@@H]1[15*]': 292, '[16*]c1ccsc1': 293, '[7*]CCCCC(=O)O': 294, '[13*][C@H]1C[C@H](O)[C@@H]([13*])O1': 295, '[9*]n1cc([16*])cn1': 296, '[8*]CCN': 297, '[16*]c1cccc(C)c1C': 298, '[16*]c1cnc(N)nc1N': 299, '[8*]CC(C)N': 300, '[16*]c1ccc(C)nc1': 301, '[13*]C1COc2ccccc2O1': 302, '[13*][C@@H]1O[C@H]([13*])[C@@H](O)[C@H](O)[C@H]1O': 303, '[15*]C1CCOCC1': 304, '[8*]C([8*])O': 305, '[5*]N1CCC([15*])(O)CC1': 306, '[13*]C1CCCCN1C': 307, '[16*]c1c(O)c2ccccc2oc1=O': 308, '[8*]CC#N': 309, '[14*]c1ncnc2[nH]ccc12': 310}\n",
      "{0: '[3*]O[3*]', 1: '[4*]C(C)C', 2: '[4*]C[8*]', 3: '[11*]SC', 4: '[3*]OC', 5: '[1*]C([6*])=O', 6: '[16*]c1ccccc1Cl', 7: '[16*]c1ccccc1', 8: '[4*]C(C)C[8*]', 9: '[5*]N[5*]', 10: '[6*]C([6*])=O', 11: '[16*]c1cccc([16*])c1', 12: '[8*]C(C)C(=O)O', 13: '[8*]C(F)(F)F', 14: '[16*]c1ccc([16*])cc1', 15: '[4*]CC', 16: '[5*]N1CCC[C@H]1[13*]', 17: '[8*]CC', 18: '[6*]C(=O)O', 19: '[16*]c1ccncc1', 20: '[8*]C[8*]', 21: '[6*]C(=O)CO', 22: '[5*]N1CCN([5*])CC1', 23: '[7*]CC[7*]', 24: '[4*]C(C)C([8*])O', 25: '[16*]c1ccc(O)cc1', 26: '[4*]CC([4*])C', 27: '[4*]CCC', 28: '[4*]CC(O)C[4*]', 29: '[4*]CC[4*]', 30: '[5*]N([5*])[5*]', 31: '[9*]n1cncn1', 32: '[16*]c1ccc(F)cc1F', 33: '[1*]C(=O)N(CCCl)N=O', 34: '[16*]c1cnc(C)nc1N', 35: '[16*]c1ccc(Cl)cc1', 36: '[16*]c1cnn(C)c1', 37: '[6*]C(N)=O', 38: '[14*]c1ncccc1[16*]', 39: '[16*]c1cccnc1', 40: '[16*]c1ccccc1[16*]', 41: '[8*]CO', 42: '[11*]S[11*]', 43: '[15*]C1CC1', 44: '[5*]N(C)C', 45: '[15*]C1CCCCC1', 46: '[1*]C(=O)CC[8*]', 47: '[5*]N1CCOCC1', 48: '[14*]c1ccco1', 49: '[5*]N([5*])C', 50: '[4*]CCC[4*]', 51: '[13*]C1CCCO1', 52: '[4*]CCCCCC', 53: '[4*]CCCC', 54: '[4*]CCCC([6*])=O', 55: '[16*]c1ccc(F)cc1', 56: '[10*]N1CCCC1=O', 57: '[16*]c1c(I)cc(I)c([16*])c1I', 58: '[16*]c1ccc2c(c1)OCO2', 59: '[1*]C(=O)C[7*]', 60: '[5*]N1CCCCC1', 61: '[7*]C[8*]', 62: '[4*]C(C)(C)C', 63: '[16*]c1ccc(I)cc1', 64: '[8*]C[C@H](N)C(=O)O', 65: '[4*]CC[8*]', 66: '[16*]c1ccc([16*])c([16*])c1', 67: '[7*]C([8*])C', 68: '[16*]c1cccc(Cl)c1Cl', 69: '[13*]C1Nc2cc(Cl)c(S(N)(=O)=O)cc2S(=O)(=O)N1', 70: '[1*]C(=O)CC', 71: '[15*][C@H]1CN2CCC1CC2', 72: '[4*]CC=C', 73: '[14*]c1ncccn1', 74: '[1*]C(C)=O', 75: '[16*]c1ccccc1O', 76: '[1*]C(=O)C([8*])([8*])O', 77: '[8*]CN', 78: '[14*]c1cccs1', 79: '[5*]N1CCCC1', 80: '[1*]C(=O)C([8*])CO', 81: '[16*]c1cccc(F)c1', 82: '[9*]n1ccc(N)nc1=O', 83: '[4*]CC#C', 84: '[16*]c1ccc(C)cc1', 85: '[6*]C(=O)CC[8*]', 86: '[6*]C(C)=O', 87: '[16*]c1ccccc1F', 88: '[3*]OP(=O)(O)O', 89: '[14*]c1ccc([16*])cn1', 90: '[5*]NC', 91: '[16*]c1cccc(C)c1', 92: '[16*]c1ccccc1C', 93: '[16*]c1c[nH]c2ccccc12', 94: '[16*]c1ccc([16*])c(F)c1', 95: '[8*]C(C)C', 96: '[16*]c1ccc(Cl)c(Cl)c1', 97: '[8*]CCC(=O)O', 98: '[16*]c1cc(I)c([16*])c(I)c1', 99: '[12*]S(=O)(=O)c1ccc([16*])cc1', 100: '[14*]c1c[nH]cn1', 101: '[4*][C@H](C)C[8*]', 102: '[16*]c1ccc(Br)cc1', 103: '[4*]CCC([8*])[8*]', 104: '[14*]c1ccccn1', 105: '[9*]n1cnc2c1c(=O)n(C)c(=O)n2C', 106: '[5*]N1CCC([15*])CC1', 107: '[12*]S(C)(=O)=O', 108: '[1*]C([1*])=O', 109: '[4*]C([8*])[8*]', 110: '[1*]C(=O)C(Cl)Cl', 111: '[16*]c1ccc(S(C)(=O)=O)cc1', 112: '[14*]c1nc2ccc([16*])cc2s1', 113: '[4*]C([8*])([8*])C', 114: '[16*]c1cc(I)c(O)c(I)c1', 115: '[4*]C(C)CC[8*]', 116: '[1*]C(=O)[C@@H]([8*])CO', 117: '[16*]c1ccc(Cl)cc1Cl', 118: '[9*]n1ccnc1', 119: '[4*]C([8*])C[8*]', 120: '[14*]c1cnccn1', 121: '[8*]CC=C', 122: '[16*]c1ccc(O)c(O)c1', 123: '[16*]c1cc(N)c(Cl)cc1[16*]', 124: '[5*][N+]([5*])(C)C', 125: '[4*]CCCCCCCCCCCC', 126: '[1*]C(=O)CCC(=O)O', 127: '[10*]N1C[C@H]([13*])OC1=O', 128: '[16*]c1c(Cl)cccc1Cl', 129: '[5*]N1c2ccccc2Sc2ccccc21', 130: '[9*]n1cc(C)c(=O)[nH]c1=O', 131: '[15*]C1CCCC1', 132: '[13*]C1SC(=O)NC1=O', 133: '[4*]CCCCC', 134: '[14*]c1ncc([16*])cn1', 135: '[4*]CC([8*])O', 136: '[15*]C1CN2CCC1CC2', 137: '[16*]c1cc([16*])cc([16*])c1', 138: '[4*][C@@H](CC[8*])C(=O)O', 139: '[1*]C(=O)[C@@H]([4*])C', 140: '[14*]c1nnn[nH]1', 141: '[3*]OC[8*]', 142: '[1*]C(=O)[C@@H](N)C(C)C', 143: '[5*]N1c2ccccc2Sc2ccc(Cl)cc21', 144: '[8*]C(C)(C)C', 145: '[4*]CC1=C(C(=O)O)N2C(=O)[C@@H]([15*])[C@H]2SC1', 146: '[16*]c1cc([16*])c([16*])c([16*])c1', 147: '[5*][N+]([5*])([5*])[5*]', 148: '[5*][N+]1(C)[C@@H]2CC[C@H]1C[C@@H]([15*])C2', 149: '[7*]C([8*])[8*]', 150: '[8*]CCC', 151: '[16*]c1ccc(F)c([16*])c1', 152: '[12*]S(=O)(=O)c1ccc(N)cc1', 153: '[16*]c1cccc(Cl)c1', 154: '[5*][N+](C)(C)C', 155: '[16*]c1ccc([16*])c(C)c1', 156: '[4*]CC(=O)O', 157: '[4*]CC([4*])C[4*]', 158: '[1*]C(=O)CCC', 159: '[8*]CC(=O)O', 160: '[8*]CCCCC', 161: '[1*]C(=O)C[8*]', 162: '[5*]NC(=N)N', 163: '[16*]c1ccc([16*])c(O)c1', 164: '[4*]CC[7*]', 165: '[16*]c1ccc2cc([16*])ccc2c1', 166: '[3*]OS(C)(=O)=O', 167: '[4*]CCCC[4*]', 168: '[7*]CC1=C(C)CCCC1(C)C', 169: '[7*]CC([7*])C', 170: '[7*]CC(=O)O', 171: '[4*]C[C@@H]([8*])O', 172: '[4*]CCCCCC[4*]', 173: '[16*]c1cccc2ccccc12', 174: '[14*]c1ccc([N+](=O)[O-])o1', 175: '[1*]C(=O)C[4*]', 176: '[1*]C(N)=O', 177: '[14*]c1nccs1', 178: '[16*]c1ccc(N)cc1', 179: '[15*][C@H]1C[C@H]2CC[C@@H](C1)N2C', 180: '[7*]CC', 181: '[4*]CC(CC)CCCC', 182: '[5*]N1c2ccccc2Sc2ccc([16*])cc21', 183: '[5*]N1CCN(C)CC1', 184: '[1*]C(=O)CN', 185: '[3*]OC(F)(F)F', 186: '[7*]C1CCC[C@]2(C)[C@@H]([15*])CC[C@@H]12', 187: '[16*]c1ccc2[nH]cc([16*])c2c1', 188: '[4*]CC([6*])=O', 189: '[5*][N+]([5*])([5*])C', 190: '[16*]c1ccc2ccccc2c1', 191: '[7*]CCCCCCCCC', 192: '[14*]c1nc2cc([16*])ccc2[nH]1', 193: '[3*]OC(F)F', 194: '[4*]CCO', 195: '[4*]CCCC[8*]', 196: '[16*]c1c(C)cccc1C', 197: '[16*]c1cc(C)c([16*])c(C)c1', 198: '[4*]C1([15*])CCN([5*])CC1', 199: '[5*]N1CCC([15*])([15*])CC1', 200: '[16*]c1ccc(O)c([16*])c1', 201: '[15*]C1([15*])C(=O)NC(=O)NC1=O', 202: '[4*]CC(C)C[4*]', 203: '[5*]N1CCCC1[13*]', 204: '[3*]O[N+](=O)[O-]', 205: '[5*][N-][5*]', 206: '[4*]CC(C)C', 207: '[1*]C(=O)C([8*])[8*]', 208: '[14*]c1nc[nH]c1C', 209: '[1*]C(=O)C1=C(C)NC(C)=C(C([1*])=O)C1[15*]', 210: '[1*]C(=O)C(C)(C)C', 211: '[8*]C([8*])C', 212: '[8*]CCCC', 213: '[16*]c1cccc(O)c1', 214: '[4*]CC1=NCCN1', 215: '[6*]C(=O)[O-]', 216: '[8*]CCO', 217: '[15*][C@H]1CC[C@H]([15*])CC1', 218: '[1*]C(=O)C([4*])(C)C', 219: '[16*]c1ccc(Cl)c([16*])c1', 220: '[5*]NC1=NCCN1', 221: '[16*]c1ccc(Cl)cc1[16*]', 222: '[7*]C(C)C', 223: '[4*]CCC([8*])([8*])O', 224: '[7*]CCC[7*]', 225: '[4*]CCC[8*]', 226: '[15*]C12CC3CC(CC(C3)C1)C2', 227: '[5*]N1CCNCC1', 228: '[6*]C(=O)CC', 229: '[9*]n1cnc2c(N)ncnc21', 230: '[4*]CC(=O)[O-]', 231: '[7*]C1CCN(C)CC1', 232: '[7*]CC[8*]', 233: '[4*][C@@H](C)C[8*]', 234: '[8*][C@@H](C)O', 235: '[4*]CCCCCCCCCCCCCCCC', 236: '[8*]CC(C)C', 237: '[8*]C([8*])[8*]', 238: '[12*]S(=O)(=O)c1ccc(C)cc1', 239: '[13*][C@@H]1N2C(=O)[C@@H]([15*])[C@H]2SC1(C)C', 240: '[1*]C(=O)C([4*])C', 241: '[1*]C(=O)C([8*])C', 242: '[8*]C(C)CCC', 243: '[15*]C1CCC1', 244: '[14*]c1ccc([14*])o1', 245: '[4*]C(C)(C)C[8*]', 246: '[9*]n1cc([16*])c(=O)c2cc(F)c([16*])cc21', 247: '[14*]c1csc([14*])n1', 248: '[16*]c1ccc(C#N)cc1', 249: '[4*][C@H]([8*])C', 250: '[4*][C@@H]([8*])C', 251: '[4*]CCCC(=O)O', 252: '[14*]c1ncc(C)c([16*])c1C', 253: '[14*]c1ncnc2cc([16*])c([16*])cc12', 254: '[16*]c1ccc([16*])c(Cl)c1', 255: '[15*]C1CCN(C)CC1', 256: '[4*]CC(F)(F)F', 257: '[9*]n1c([N+](=O)[O-])cnc1C', 258: '[13*][C@@H]1O[C@H]([13*])[C@@H](O)[C@@H]1O', 259: '[4*]CCCl', 260: '[13*][C@@H]1O[C@H]([13*])[C@@H](O)[C@H]1O', 261: '[14*]c1cc([16*])ccn1', 262: '[1*]C(=O)CC[4*]', 263: '[4*]CCC[7*]', 264: '[16*]c1cccc([16*])c1[16*]', 265: '[15*]C1CCNCC1', 266: '[14*]c1noc([14*])n1', 267: '[4*]CCCCCCCC', 268: '[1*]C(=O)C([4*])[8*]', 269: '[5*][N+]1(C)CCCC1', 270: '[16*]c1ccc(S(N)(=O)=O)cc1', 271: '[16*]c1ccc([N+](=O)[O-])cc1', 272: '[1*]C(=O)C(C)C', 273: '[6*]C(=O)NO', 274: '[9*]n1cnc2c(=O)[nH]c(N)nc21', 275: '[4*]CCN', 276: '[4*]CC(O)CO', 277: '[16*]c1ccnc2cc(Cl)ccc12', 278: '[4*]C(C)(C)C(=O)O', 279: '[16*]c1c(I)c([16*])c(I)c([16*])c1I', 280: '[9*]n1c(=O)c([16*])c(C)n1C', 281: '[4*]C[C@H](N)C(=O)O', 282: '[14*]c1ccc([14*])nn1', 283: '[7*]C1c2ccccc2CCc2ccccc21', 284: '[8*]CC[8*]', 285: '[4*]CCC(C)C', 286: '[1*]C(=O)[C@@H]([8*])N', 287: '[4*]CCC([8*])([8*])C(N)=O', 288: '[11*]SS[11*]', 289: '[13*]C1CCCCN1', 290: '[8*]CC1=NCCN1', 291: '[11*]SC(C)=O', 292: '[15*][C@H]1[C@H](O)CC(=O)[C@@H]1[15*]', 293: '[16*]c1ccsc1', 294: '[7*]CCCCC(=O)O', 295: '[13*][C@H]1C[C@H](O)[C@@H]([13*])O1', 296: '[9*]n1cc([16*])cn1', 297: '[8*]CCN', 298: '[16*]c1cccc(C)c1C', 299: '[16*]c1cnc(N)nc1N', 300: '[8*]CC(C)N', 301: '[16*]c1ccc(C)nc1', 302: '[13*]C1COc2ccccc2O1', 303: '[13*][C@@H]1O[C@H]([13*])[C@@H](O)[C@H](O)[C@H]1O', 304: '[15*]C1CCOCC1', 305: '[8*]C([8*])O', 306: '[5*]N1CCC([15*])(O)CC1', 307: '[13*]C1CCCCN1C', 308: '[16*]c1c(O)c2ccccc2oc1=O', 309: '[8*]CC#N', 310: '[14*]c1ncnc2[nH]ccc12'}\n"
     ]
    }
   ],
   "source": [
    "print(frag2idx)\n",
    "print(idx2frag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "61a1aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class JTVDataset(Dataset):\n",
    "    def __init__(self, fragment_indices_list):\n",
    "        self.data = fragment_indices_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return as tensor\n",
    "        fragments = self.data[idx]\n",
    "        return torch.tensor(fragments, dtype=torch.long)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    # batch is a list of tensors (fragment indices)\n",
    "    lengths = [len(x) for x in batch]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    padded_batch = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, seq in enumerate(batch):\n",
    "        padded_batch[i, :lengths[i]] = seq\n",
    "    \n",
    "    return padded_batch, torch.tensor(lengths, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "231b68d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining molecules: 7095\n"
     ]
    }
   ],
   "source": [
    "# Remove molecules that have zero fragments\n",
    "df = df[df['fragment_indices'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "\n",
    "print(\"Remaining molecules:\", len(df))\n",
    "dataset = JTVDataset(df['fragment_indices'].tolist())\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c463ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([16, 7])\n",
      "Lengths: tensor([6, 3, 1, 7, 5, 2, 7, 2, 3, 3, 5, 6, 1, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch, lengths in dataloader:\n",
    "    print(\"Batch shape:\", batch.shape)\n",
    "    print(\"Lengths:\", lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5f7c01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(vocab_fragments)           # from your vocab (frag2idx)\n",
    "embed_dim = 128                             # fragment embedding dim\n",
    "enc_hidden = 256                            # encoder GRU hidden size\n",
    "dec_hidden = 256                            # decoder GRU hidden size\n",
    "z_dim = 64                                  # latent dim\n",
    "pad_idx = 0                                 # padding index (we used 0 padding in collate)\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "epochs = 80\n",
    "teacher_forcing_start = 1.0                 # start prob\n",
    "teacher_forcing_end = 0.5                   # end prob after anneal\n",
    "tf_anneal_epochs = 30\n",
    "kl_anneal_epochs = 60\n",
    "beta = 1.0                                  # final KL weight (you can increase to >1)\n",
    "max_dec_len = 32                            # safety cap for decoding length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3eb5a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, enc_hidden, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, enc_hidden, batch_first=True)\n",
    "        self.enc_hidden = enc_hidden\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        x: [B, L] LongTensor (padded fragment indices)\n",
    "        lengths: [B] LongTensor\n",
    "        returns: last_hidden: [B, enc_hidden]\n",
    "        \"\"\"\n",
    "        emb = self.embed(x)  # [B, L, E]\n",
    "        # pack\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, h_n = self.gru(packed)  # h_n: [1, B, enc_hidden]\n",
    "        return h_n.squeeze(0)  # [B, enc_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5a03a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentHead(nn.Module):\n",
    "    def __init__(self, enc_hidden, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear_mu = nn.Linear(enc_hidden, z_dim)\n",
    "        self.linear_logvar = nn.Linear(enc_hidden, z_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        mu = self.linear_mu(h)\n",
    "        logvar = self.linear_logvar(h)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aba7d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = (0.5 * logvar).exp()\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ab34865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, dec_hidden, z_dim, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.z_to_hidden = nn.Linear(z_dim, dec_hidden)\n",
    "        # GRU input: embed_dim + z_dim\n",
    "        self.gru = nn.GRU(embed_dim + z_dim, dec_hidden, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden, vocab_size)\n",
    "\n",
    "    def init_hidden_from_z(self, z):\n",
    "        # z: [B, z_dim] -> [1, B, dec_hidden]\n",
    "        return torch.tanh(self.z_to_hidden(z)).unsqueeze(0)\n",
    "\n",
    "    def forward(self, inputs, z, hidden=None):\n",
    "        # Full sequence forward (teacher forcing)\n",
    "        B, L = inputs.size()\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden_from_z(z)\n",
    "\n",
    "        emb = self.embed(inputs)                               # [B,L,E]\n",
    "        z_exp = z.unsqueeze(1).expand(-1, L, -1)         # [B,L,z_dim]\n",
    "        gru_input = torch.cat([emb, z_exp], dim=-1)      # [B,L,E+z]\n",
    "        out, hidden = self.gru(gru_input, hidden)        # [B,L,H]\n",
    "        logits = self.out(out)                            # [B,L,vocab]\n",
    "        return logits, hidden\n",
    "\n",
    "    def step(self, input_token, z, hidden=None):\n",
    "        \"\"\"\n",
    "        Single time-step for autoregressive inference.\n",
    "        input_token: [B] long\n",
    "        z: [B, z_dim]\n",
    "        hidden: previous hidden\n",
    "        returns logits [B, vocab], new_hidden\n",
    "        \"\"\"\n",
    "        emb = self.embed(input_token).unsqueeze(1)  # [B,1,E]\n",
    "        gru_in = torch.cat([emb, z.unsqueeze(1)], dim=-1)  # [B,1,E+z]\n",
    "        out, h_n = self.gru(gru_in, hidden)  # out: [B,1,dec_hidden]\n",
    "        logits = self.out(out.squeeze(1))  # [B, vocab]\n",
    "        return logits, h_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19a2db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, enc_hidden, dec_hidden, z_dim, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.encoder = TreeEncoder(vocab_size, embed_dim, enc_hidden, pad_idx)\n",
    "        self.latent = LatentHead(enc_hidden, z_dim)\n",
    "        self.decoder = TreeDecoder(vocab_size, embed_dim, dec_hidden, z_dim, pad_idx)\n",
    "\n",
    "    def forward(self, x, lengths, tf_prob=1.0):\n",
    "        \"\"\"\n",
    "        x: [B, L] padded target sequences (we will use teacher forcing)\n",
    "        lengths: [B] actual lengths\n",
    "        tf_prob: teacher forcing probability (0..1)\n",
    "        Returns: logits [B, L, V], mu, logvar\n",
    "        \"\"\"\n",
    "        h_enc = self.encoder(x, lengths)                    # [B, enc_hidden]\n",
    "        mu, logvar = self.latent(h_enc)                     # [B, z_dim]\n",
    "        z = reparameterize(mu, logvar).to(device)           # [B, z_dim]\n",
    "\n",
    "        # Decoder with teacher forcing:\n",
    "        # For teacher forcing we input the target sequence as inputs (shifted if you want SOS)\n",
    "        _, hidden = self.decoder(x, z)                       # predict tokens given the inputs (simpler)\n",
    "        return mu, logvar, z, hidden\n",
    "\n",
    "    def sample_from_z(self, z, sos_idx=1, max_len=32):\n",
    "        \"\"\"\n",
    "        z: [B, z_dim] latent\n",
    "        returns: generated indices list (B x <=max_len)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        B = z.size(0)\n",
    "        with torch.no_grad():\n",
    "            # start token: we will assume user uses a special sos index; if none, use first fragment in vocab\n",
    "            input_tok = torch.full((B,), sos_idx, dtype=torch.long, device=z.device)  # [B]\n",
    "            hidden = None\n",
    "            generated = []\n",
    "            for t in range(max_len):\n",
    "                logits, hidden = self.decoder.step(input_tok, z, hidden)  # [B, vocab]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # sample or argmax; use sampling to get diverse outputs\n",
    "                input_tok = torch.multinomial(probs, num_samples=1).squeeze(1)  # [B]\n",
    "                generated.append(input_tok.unsqueeze(1))\n",
    "            gen = torch.cat(generated, dim=1)  # [B, max_len]\n",
    "        return gen  # indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ee5f2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss funtions\n",
    "def compute_recon_loss(logits, targets, lengths, pad_idx=0):\n",
    "    \"\"\"\n",
    "    logits: [B, L, V]\n",
    "    targets: [B, L]\n",
    "    lengths: [B]\n",
    "    returns: scalar recon loss averaged per batch (sum over real tokens / batch)\n",
    "    \"\"\"\n",
    "    B, L, V = logits.shape\n",
    "    logits_flat = logits.view(B * L, V)\n",
    "    targets_flat = targets.view(B * L)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=pad_idx, reduction='sum')\n",
    "    # normalize by batch size (or number of real tokens if you prefer)\n",
    "    return loss / B\n",
    "\n",
    "def compute_kl_freebits(mu, logvar, free_bits=0.5):\n",
    "    # kl per example (scalar)\n",
    "    kl_per = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)  # [B]\n",
    "    kl_per_clamped = torch.clamp(kl_per - free_bits, min=0.0) + free_bits  # ensures at least free_bits\n",
    "    return kl_per_clamped.mean()\n",
    "\n",
    "def linear_anneal(step, total_steps, start=0.0, end=1.0):\n",
    "    frac = min(1.0, step / float(total_steps))\n",
    "    return start + frac * (end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e4896eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 8.6769 | Recon 8.3567 | KL 19.2122 | KLw 0.017 | TF 0.983\n",
      "Epoch 002 | Loss 4.7675 | Recon 4.1914 | KL 17.2827 | KLw 0.033 | TF 0.967\n",
      "Epoch 003 | Loss 3.3333 | Recon 2.7830 | KL 11.0065 | KLw 0.050 | TF 0.950\n",
      "Epoch 004 | Loss 2.4899 | Recon 2.0481 | KL 6.6279 | KLw 0.067 | TF 0.933\n",
      "Epoch 005 | Loss 1.8450 | Recon 1.5150 | KL 3.9604 | KLw 0.083 | TF 0.917\n",
      "Epoch 006 | Loss 1.4065 | Recon 1.1790 | KL 2.2757 | KLw 0.100 | TF 0.900\n",
      "Epoch 007 | Loss 1.0533 | Recon 0.9029 | KL 1.2893 | KLw 0.117 | TF 0.883\n",
      "Epoch 008 | Loss 0.7819 | Recon 0.6757 | KL 0.7964 | KLw 0.133 | TF 0.867\n",
      "Epoch 009 | Loss 0.6243 | Recon 0.5333 | KL 0.6063 | KLw 0.150 | TF 0.850\n",
      "Epoch 010 | Loss 0.5132 | Recon 0.4234 | KL 0.5386 | KLw 0.167 | TF 0.833\n",
      "Epoch 011 | Loss 0.4227 | Recon 0.3282 | KL 0.5154 | KLw 0.183 | TF 0.817\n",
      "Epoch 012 | Loss 0.3853 | Recon 0.2838 | KL 0.5076 | KLw 0.200 | TF 0.800\n",
      "Epoch 013 | Loss 0.3697 | Recon 0.2603 | KL 0.5050 | KLw 0.217 | TF 0.783\n",
      "Epoch 014 | Loss 0.3793 | Recon 0.2619 | KL 0.5033 | KLw 0.233 | TF 0.767\n",
      "Epoch 015 | Loss 0.3719 | Recon 0.2462 | KL 0.5030 | KLw 0.250 | TF 0.750\n",
      "Epoch 016 | Loss 0.3603 | Recon 0.2261 | KL 0.5030 | KLw 0.267 | TF 0.733\n",
      "Epoch 017 | Loss 0.3616 | Recon 0.2194 | KL 0.5021 | KLw 0.283 | TF 0.717\n",
      "Epoch 018 | Loss 0.3530 | Recon 0.2026 | KL 0.5014 | KLw 0.300 | TF 0.700\n",
      "Epoch 019 | Loss 0.3376 | Recon 0.1788 | KL 0.5015 | KLw 0.317 | TF 0.683\n",
      "Epoch 020 | Loss 0.3509 | Recon 0.1838 | KL 0.5012 | KLw 0.333 | TF 0.667\n",
      "Epoch 021 | Loss 0.3483 | Recon 0.1730 | KL 0.5010 | KLw 0.350 | TF 0.650\n",
      "Epoch 022 | Loss 0.3488 | Recon 0.1652 | KL 0.5008 | KLw 0.367 | TF 0.633\n",
      "Epoch 023 | Loss 0.4003 | Recon 0.2081 | KL 0.5014 | KLw 0.383 | TF 0.617\n",
      "Epoch 024 | Loss 0.3834 | Recon 0.1831 | KL 0.5008 | KLw 0.400 | TF 0.600\n",
      "Epoch 025 | Loss 0.4000 | Recon 0.1914 | KL 0.5008 | KLw 0.417 | TF 0.583\n",
      "Epoch 026 | Loss 0.3726 | Recon 0.1558 | KL 0.5005 | KLw 0.433 | TF 0.567\n",
      "Epoch 027 | Loss 0.4048 | Recon 0.1796 | KL 0.5004 | KLw 0.450 | TF 0.550\n",
      "Epoch 028 | Loss 0.4173 | Recon 0.1837 | KL 0.5006 | KLw 0.467 | TF 0.533\n",
      "Epoch 029 | Loss 0.4342 | Recon 0.1924 | KL 0.5003 | KLw 0.483 | TF 0.517\n",
      "Epoch 030 | Loss 0.4168 | Recon 0.1666 | KL 0.5004 | KLw 0.500 | TF 0.500\n",
      "Epoch 031 | Loss 0.4583 | Recon 0.1997 | KL 0.5003 | KLw 0.517 | TF 0.500\n",
      "Epoch 032 | Loss 0.4499 | Recon 0.1831 | KL 0.5003 | KLw 0.533 | TF 0.500\n",
      "Epoch 033 | Loss 0.4462 | Recon 0.1711 | KL 0.5003 | KLw 0.550 | TF 0.500\n",
      "Epoch 034 | Loss 0.4364 | Recon 0.1530 | KL 0.5002 | KLw 0.567 | TF 0.500\n",
      "Epoch 035 | Loss 0.4519 | Recon 0.1601 | KL 0.5002 | KLw 0.583 | TF 0.500\n",
      "Epoch 036 | Loss 0.4321 | Recon 0.1321 | KL 0.5001 | KLw 0.600 | TF 0.500\n",
      "Epoch 037 | Loss 0.4329 | Recon 0.1245 | KL 0.5001 | KLw 0.617 | TF 0.500\n",
      "Epoch 038 | Loss 0.4413 | Recon 0.1246 | KL 0.5001 | KLw 0.633 | TF 0.500\n",
      "Epoch 039 | Loss 0.4836 | Recon 0.1585 | KL 0.5002 | KLw 0.650 | TF 0.500\n",
      "Epoch 040 | Loss 0.4944 | Recon 0.1609 | KL 0.5002 | KLw 0.667 | TF 0.500\n",
      "Epoch 041 | Loss 0.4849 | Recon 0.1431 | KL 0.5001 | KLw 0.683 | TF 0.500\n",
      "Epoch 042 | Loss 0.4822 | Recon 0.1321 | KL 0.5001 | KLw 0.700 | TF 0.500\n",
      "Epoch 043 | Loss 0.4783 | Recon 0.1199 | KL 0.5001 | KLw 0.717 | TF 0.500\n",
      "Epoch 044 | Loss 0.5097 | Recon 0.1429 | KL 0.5002 | KLw 0.733 | TF 0.500\n",
      "Epoch 045 | Loss 0.5107 | Recon 0.1356 | KL 0.5001 | KLw 0.750 | TF 0.500\n",
      "Epoch 046 | Loss 0.5267 | Recon 0.1433 | KL 0.5001 | KLw 0.767 | TF 0.500\n",
      "Epoch 047 | Loss 0.5318 | Recon 0.1401 | KL 0.5000 | KLw 0.783 | TF 0.500\n",
      "Epoch 048 | Loss 0.5320 | Recon 0.1320 | KL 0.5000 | KLw 0.800 | TF 0.500\n",
      "Epoch 049 | Loss 0.5373 | Recon 0.1288 | KL 0.5001 | KLw 0.817 | TF 0.500\n",
      "Epoch 050 | Loss 0.5352 | Recon 0.1185 | KL 0.5000 | KLw 0.833 | TF 0.500\n",
      "Epoch 051 | Loss 0.5477 | Recon 0.1226 | KL 0.5000 | KLw 0.850 | TF 0.500\n",
      "Epoch 052 | Loss 0.5771 | Recon 0.1436 | KL 0.5001 | KLw 0.867 | TF 0.500\n",
      "Epoch 053 | Loss 0.6025 | Recon 0.1607 | KL 0.5001 | KLw 0.883 | TF 0.500\n",
      "Epoch 054 | Loss 0.5649 | Recon 0.1149 | KL 0.5001 | KLw 0.900 | TF 0.500\n",
      "Epoch 055 | Loss 0.5883 | Recon 0.1299 | KL 0.5001 | KLw 0.917 | TF 0.500\n",
      "Epoch 056 | Loss 0.5833 | Recon 0.1165 | KL 0.5001 | KLw 0.933 | TF 0.500\n",
      "Epoch 057 | Loss 0.5871 | Recon 0.1120 | KL 0.5000 | KLw 0.950 | TF 0.500\n",
      "Epoch 058 | Loss 0.6008 | Recon 0.1174 | KL 0.5000 | KLw 0.967 | TF 0.500\n",
      "Epoch 059 | Loss 0.6064 | Recon 0.1146 | KL 0.5001 | KLw 0.983 | TF 0.500\n",
      "Epoch 060 | Loss 0.6174 | Recon 0.1173 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 061 | Loss 0.6063 | Recon 0.1062 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 062 | Loss 0.5921 | Recon 0.0920 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 063 | Loss 0.6059 | Recon 0.1058 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 064 | Loss 0.6070 | Recon 0.1069 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 065 | Loss 0.6232 | Recon 0.1231 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 066 | Loss 0.6189 | Recon 0.1188 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 067 | Loss 0.6175 | Recon 0.1174 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 068 | Loss 0.6165 | Recon 0.1164 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 069 | Loss 0.6114 | Recon 0.1113 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 070 | Loss 0.6044 | Recon 0.1044 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 071 | Loss 0.6195 | Recon 0.1194 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 072 | Loss 0.6117 | Recon 0.1117 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 073 | Loss 0.6197 | Recon 0.1197 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 074 | Loss 0.6163 | Recon 0.1163 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 075 | Loss 0.5933 | Recon 0.0933 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 076 | Loss 0.5867 | Recon 0.0867 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 077 | Loss 0.6059 | Recon 0.1058 | KL 0.5000 | KLw 1.000 | TF 0.500\n",
      "Epoch 078 | Loss 0.6268 | Recon 0.1267 | KL 0.5001 | KLw 1.000 | TF 0.500\n",
      "Epoch 079 | Loss 0.6142 | Recon 0.1140 | KL 0.5002 | KLw 1.000 | TF 0.500\n",
      "Epoch 080 | Loss 0.6234 | Recon 0.1233 | KL 0.5000 | KLw 1.000 | TF 0.500\n"
     ]
    }
   ],
   "source": [
    "model = TreeVAE(vocab_size=vocab_size, embed_dim=embed_dim, enc_hidden=enc_hidden,\n",
    "                dec_hidden=dec_hidden, z_dim=z_dim, pad_idx=pad_idx).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# assuming dataloader yields (padded_batch, lengths)\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_recon = 0.0\n",
    "    running_kl = 0.0\n",
    "    # compute annealed scheduler values\n",
    "    kl_w = linear_anneal(epoch, kl_anneal_epochs, start=0.0, end=beta)  # epoch-based\n",
    "    # teacher forcing probability anneal\n",
    "    tf_prob = linear_anneal(epoch, tf_anneal_epochs, start=teacher_forcing_start, end=teacher_forcing_end)\n",
    "\n",
    "    for batch_idx, (padded_batch, lengths) in enumerate(dataloader):\n",
    "        padded_batch = padded_batch.to(device)   # [B, L]\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mu, logvar, z, hidden0 = model(padded_batch, lengths)  # forward returns these now\n",
    "        B, T = padded_batch.size()\n",
    "        \n",
    "        input_tok = padded_batch[:, 0].to(device)  # or special SOS token\n",
    "        hidden = hidden0\n",
    "        logits_list = []\n",
    "        for t in range(0, padded_batch.size(1)):\n",
    "            logits_t, hidden = model.decoder.step(input_tok, z, hidden)\n",
    "            logits_list.append(logits_t.unsqueeze(1))\n",
    "            # scheduled sampling decision\n",
    "            use_teacher = (torch.rand(B, device=device) < tf_prob)\n",
    "            gt_next = padded_batch[:, t].to(device)\n",
    "            sampled = torch.multinomial(F.softmax(logits_t, dim=-1), 1).squeeze(1)\n",
    "            input_tok = torch.where(use_teacher, gt_next, sampled)\n",
    "\n",
    "        logits = torch.cat(logits_list, dim=1)\n",
    "        recon_loss = compute_recon_loss(logits, padded_batch, lengths, pad_idx=pad_idx)\n",
    "        kl_loss = compute_kl_freebits(mu, logvar, free_bits=0.5)  # optional free-bits\n",
    "        loss = recon_loss + kl_w * kl_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += float(loss.item())\n",
    "        running_recon += float(recon_loss.item())\n",
    "        running_kl += float(kl_loss.item())\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_recon = running_recon / len(dataloader)\n",
    "    avg_kl = running_kl / len(dataloader)\n",
    "    print(f\"Epoch {epoch:03d} | Loss {avg_loss:.4f} | Recon {avg_recon:.4f} | KL {avg_kl:.4f} | KLw {kl_w:.3f} | TF {tf_prob:.3f}\")\n",
    "\n",
    "    '''# optional: sample some molecules from learned prior every few epochs\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z_prior = torch.randn(8, z_dim).to(device)\n",
    "            samples_idx = model.sample_from_z(z_prior, sos_idx=1, max_len=12)  # [8, L]\n",
    "            # convert indices to fragment SMILES:\n",
    "            samples_fraglists = []\n",
    "            for row in samples_idx.cpu().numpy():\n",
    "                # convert to fragment strings: break at pad or special tokens if used\n",
    "                fraglist = [idx2frag[int(i)] for i in row if int(i) in idx2frag]\n",
    "                samples_fraglists.append(fraglist)\n",
    "            print(\"Sampled fragments (example):\", samples_fraglists[:4])'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8674abb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled fragments (example): [['[4*]CCN', '[16*]c1ccc(S(C)(=O)=O)cc1', '[4*]CCN', '[4*]CCN', '[4*]CCN', '[4*]CCN']]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z_prior = torch.randn(1, z_dim).to(device)\n",
    "    samples_idx = model.sample_from_z(z_prior, sos_idx=1, max_len=6)  # [8, L]\n",
    "    # convert indices to fragment SMILES:\n",
    "    samples_fraglists = []\n",
    "    for row in samples_idx.cpu().numpy():\n",
    "        # convert to fragment strings: break at pad or special tokens if used\n",
    "        fraglist = [idx2frag[int(i)] for i in row if int(i) in idx2frag]\n",
    "        samples_fraglists.append(fraglist)\n",
    "    print(\"Sampled fragments (example):\", samples_fraglists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9f6d9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "\n",
    "\n",
    "def decode_fragments_to_smiles(batch_indices, vocab):\n",
    "    \"\"\"\n",
    "    Convert batch of fragment indices to SMILES\n",
    "    batch_indices: [B, max_len] LongTensor\n",
    "    vocab: list of fragment strings\n",
    "    \"\"\"\n",
    "    smiles_list = []\n",
    "    for seq in batch_indices:\n",
    "        fragments = [vocab[idx] for idx in seq.tolist()]\n",
    "        # Simple concatenation (for demo); real JT-VAE uses tree assembly\n",
    "        mol_str = \".\".join(fragments)   # dot-join fragments as placeholder\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(mol_str)\n",
    "            if mol is not None:\n",
    "                smiles_list.append(Chem.MolToSmiles(mol))\n",
    "            else:\n",
    "                smiles_list.append(mol_str)  # fallback\n",
    "        except:\n",
    "            smiles_list.append(mol_str)\n",
    "    return smiles_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3e66396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1*]C([1*])=O.[1*]C([1*])=O.[1*]C([1*])=O.[4*]CCN.[4*]CCN.[5*]N[5*]\n",
      "['[1*]C([1*])=O.[1*]C([1*])=O.[1*]C([1*])=O.[4*]CCN.[4*]CCN.[5*]N[5*]']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "z = torch.randn(1, model.decoder.z_to_hidden.in_features, device=device)\n",
    "sampled_indices = model.sample_from_z(z, sos_idx=1, max_len=6)  # [5, 12]\n",
    "\n",
    "smiles_out = decode_fragments_to_smiles(sampled_indices, idx2frag)\n",
    "for s in smiles_out:\n",
    "    print(s)\n",
    "print(smiles_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ee6c0ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'NeedsUpdatePropertyCache'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m img = \u001b[43mDraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMolToImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\drug_agent\\.venv\\Lib\\site-packages\\rdkit\\Chem\\Draw\\__init__.py:104\u001b[39m, in \u001b[36mMolToImage\u001b[39m\u001b[34m(mol, size, kekulize, wedgeBonds, fitImage, options, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(rdMolDraw2D, \u001b[33m'\u001b[39m\u001b[33mMolDraw2DCairo\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMolToImage requires that the RDKit be built with Cairo support\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_moltoimg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhighlightAtoms\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlegend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mhighlightBonds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhighlightBonds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m                                           \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrawOptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkekulize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkekulize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mwedgeBonds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwedgeBonds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhighlightColor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhighlightColor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TYC\\Desktop\\python code\\drug_agent\\.venv\\Lib\\site-packages\\rdkit\\Chem\\Draw\\__init__.py:226\u001b[39m, in \u001b[36m_moltoimg\u001b[39m\u001b[34m(mol, sz, highlights, legend, returnPNG, drawOptions, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_moltoimg\u001b[39m(mol, sz, highlights, legend, returnPNG=\u001b[38;5;28;01mFalse\u001b[39;00m, drawOptions=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmol\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNeedsUpdatePropertyCache\u001b[49m():\n\u001b[32m    227\u001b[39m     mol.UpdatePropertyCache(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    229\u001b[39m   kekulize = shouldKekulize(mol, kwargs.get(\u001b[33m'\u001b[39m\u001b[33mkekulize\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'NeedsUpdatePropertyCache'"
     ]
    }
   ],
   "source": [
    "img = Draw.MolToImage(smiles_out)\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
